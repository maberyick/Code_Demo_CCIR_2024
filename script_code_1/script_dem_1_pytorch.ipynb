{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688b3350-c17f-46cb-9370-0341b5e4ad23",
   "metadata": {},
   "source": [
    "# Train a model for classifying tissue samples into benign vs malign (Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9411c72-065b-4cdf-9420-6d07685f60ef",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b33f0d-e5c7-43df-bbe8-c916882916e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow version: 9.4.0\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ca917-aba1-4995-a5a3-0adb0859d367",
   "metadata": {},
   "source": [
    "## Get the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2812368-7a0a-4e11-ab38-54e6e98f51df",
   "metadata": {},
   "source": [
    "### Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb10980-850d-448f-8320-28587b703453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_01.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_02.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_03.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_04.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_05.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_06.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_07.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_08.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_09.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_10.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_11.tar.gz...\n",
      "downloading/home/crisrbarreram/Documents/ccir_demo/imgs/images_12.tar.gz...\n",
      "Download complete. Please check the checksums\n"
     ]
    }
   ],
   "source": [
    "# Download the 56 zip files in Images_png in batches\n",
    "# URLs for the zip files\n",
    "links = [\n",
    "    'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
    "    'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
    "    'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
    "\t'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
    "    'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
    "\t'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
    "\t'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
    "    'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
    "\t'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
    "\t'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
    "\t'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
    "\t'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
    "]\n",
    "\n",
    "\n",
    "for idx, link in enumerate(links):\n",
    "    fn = '/home/crisrbarreram/Documents/ccir_demo/imgs/images_%02d.tar.gz' % (idx+1)\n",
    "    print('downloading'+fn+'...')\n",
    "    urllib.request.urlretrieve(link, fn)  # download the zip file\n",
    "\n",
    "\n",
    "print(\"Download complete. Please check the checksums\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf4708-15c0-4897-af03-a726ae7f76b8",
   "metadata": {},
   "source": [
    "### Define the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d335f0-1491-4b8f-914c-7ea2703ac7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = '/home/cbarr23/Documents/ccir_demo/imgs/'\n",
    "csv_path = '/home/cbarr23/Documents/ccir_demo/Data_Entry_2017_v2020.csv'\n",
    "output_dir = '/home/cbarr23/Documents/ccir_demo/processed/'\n",
    "image_dir = '/home/cbarr23/Documents/ccir_demo/imgs/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63e0b2-60cf-4139-b379-367df7b1ff75",
   "metadata": {},
   "source": [
    "### Unpack the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a82f601-0620-40bb-94e3-3c99f01df01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacked images_01.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_01\n",
      "Unpacked images_02.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_02\n",
      "Unpacked images_03.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_03\n",
      "Unpacked images_05.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_05\n",
      "Unpacked images_10.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_10\n",
      "Unpacked images_09.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_09\n",
      "Unpacked images_12.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_12\n",
      "Unpacked images_06.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_06\n",
      "Unpacked images_07.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_07\n",
      "Unpacked images_04.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_04\n",
      "Unpacked images_08.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_08\n",
      "Unpacked images_11.tar.gz into /home/cbarr23/Documents/ccir_demo/imgs/images_11\n"
     ]
    }
   ],
   "source": [
    "# Unpack all .tar.gz files into individual subdirectories\n",
    "tar_files = [f for f in os.listdir(data_dir) if f.endswith('.tar.gz')]\n",
    "for tar_file in tar_files:\n",
    "    tar_path = os.path.join(data_dir, tar_file)\n",
    "    subdir = os.path.join(data_dir, tar_file[:-7])  # Create subdirectory based on tar file name\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=subdir)\n",
    "    print(f'Unpacked {tar_file} into {subdir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f6633e-d2c2-4595-bb19-190d090fced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for processed data\n",
    "os.makedirs(os.path.join(output_dir, 'train/normal'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'train/pneumonia'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'test/normal'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'test/pneumonia'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783dcc76-0475-4b31-9951-fe5db6205e2b",
   "metadata": {},
   "source": [
    "### Load the csv file for identifying the labels for normal vs pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f0d7f7e-9fb0-48c3-8be7-2738d0b0d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab7809-e12d-48a5-a7df-b4bfa512794e",
   "metadata": {},
   "source": [
    "### filter out for Pneumonia vs Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "285f0bd6-5554-4846-8e97-093bdeacc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV and filter images with labels 'No Finding' and 'Pneumonia'\n",
    "df = pd.read_csv(csv_path)\n",
    "filtered_df = df[df['Finding Labels'].isin(['No Finding', 'Pneumonia'])].copy()\n",
    "\n",
    "# Map 'No Finding' to 'normal' and 'Pneumonia' to 'pneumonia'\n",
    "filtered_df.loc[:, 'label'] = filtered_df['Finding Labels'].map({'No Finding': 'normal', 'Pneumonia': 'pneumonia'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ae5d2-5a66-4bef-bbf1-3d9ef7bb8dce",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20b35cc1-f2a8-4180-91a6-db3b80b16b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(filtered_df, test_size=0.2, stratify=filtered_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61ab3a-d5c9-4ff6-9460-1b575554d901",
   "metadata": {},
   "source": [
    "### copy images to corresponding folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f55197af-cfbb-4b2c-b3a7-6461cf4d9a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been successfully filtered and organized.\n"
     ]
    }
   ],
   "source": [
    "# Function to copy images to their respective directories\n",
    "def copy_images(df, split):\n",
    "    for _, row in df.iterrows():\n",
    "        label = row['label']\n",
    "        image_path = os.path.join(image_dir, row['Image Index'])\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, os.path.join(output_dir, split, label, row['Image Index']))\n",
    "\n",
    "\n",
    "# Copy images to train and test directories\n",
    "copy_images(train_df, 'train')\n",
    "copy_images(test_df, 'test')\n",
    "\n",
    "\n",
    "print(\"Images have been successfully filtered and organized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad47e74-951e-472d-b097-1a165a64eb05",
   "metadata": {},
   "source": [
    "## Get the model ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668947d-b868-4b62-8317-f1d0f68703a3",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5e544f6-386a-4743-8c1c-8e4f93fc0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6f8d9-abac-4ca0-b0e1-77307ade3a16",
   "metadata": {},
   "source": [
    "### Generate the data for the DL model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ea8495-5332-46f3-9db4-04c4f34c08db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48546 images belonging to 2 classes.\n",
      "Found 12137 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define image transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data generators\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(output_dir, 'train'), transform=train_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(output_dir, 'test'), transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c86f46-1480-4a76-801a-c6fc357f1b75",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94a046ab-1104-4bc5-b8bf-2ae09e3876bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * (img_height // 8) * (img_width // 8), 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * (img_height // 8) * (img_width // 8))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4eb9e-5098-463b-b1ca-37ce92324162",
   "metadata": {},
   "source": [
    "### Combine the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e6b280d-39cd-4b31-afc5-66ab1741d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653a8c6-af70-4e4b-ba63-88bb5f91102e",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2789c2-adc3-4653-bfda-d20012480cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 07:14:22.447530: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 07:25:43.732492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - 805s 530ms/step - loss: 0.0423 - accuracy: 0.9941 - val_loss: 0.0333 - val_accuracy: 0.9947\n",
      "Epoch 2/10\n",
      "1518/1518 [==============================] - 803s 529ms/step - loss: 0.0359 - accuracy: 0.9947 - val_loss: 0.0358 - val_accuracy: 0.9947\n",
      "Epoch 3/10\n",
      "1518/1518 [==============================] - 801s 527ms/step - loss: 0.0354 - accuracy: 0.9947 - val_loss: 0.0335 - val_accuracy: 0.9947\n",
      "Epoch 4/10\n",
      "1518/1518 [==============================] - 797s 525ms/step - loss: 0.0359 - accuracy: 0.9947 - val_loss: 0.0329 - val_accuracy: 0.9947\n",
      "Epoch 5/10\n",
      "1518/1518 [==============================] - 805s 530ms/step - loss: 0.0349 - accuracy: 0.9947 - val_loss: 0.0338 - val_accuracy: 0.9947\n",
      "Epoch 6/10\n",
      "1518/1518 [==============================] - 807s 531ms/step - loss: 0.0489 - accuracy: 0.9946 - val_loss: 0.0331 - val_accuracy: 0.9947\n",
      "Epoch 7/10\n",
      "1518/1518 [==============================] - 796s 525ms/step - loss: 0.0352 - accuracy: 0.9947 - val_loss: 0.0336 - val_accuracy: 0.9947\n",
      "Epoch 8/10\n",
      "1518/1518 [==============================] - 801s 527ms/step - loss: 0.0354 - accuracy: 0.9947 - val_loss: 0.0334 - val_accuracy: 0.9947\n",
      "Epoch 9/10\n",
      "1518/1518 [==============================] - 804s 530ms/step - loss: 0.0346 - accuracy: 0.9947 - val_loss: 0.0352 - val_accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "1280/1518 [========================>.....] - ETA: 1:45 - loss: 0.0351 - accuracy: 0.9946"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7c764-b355-494f-889c-f6e81c3a47b7",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75e57a-5ec7-46c6-96f3-64951c2da6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), '/home/cbarr23/Documents/ccir_demo/model/cxr_classification_epoch_10_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa9bac-94d2-4863-89cf-994cac3fc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('/home/cbarr23/Documents/ccir_demo/model/cxr_classification_epoch_10_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Grad-CAM function\n",
    "def get_gradcam_heatmap(model, img_tensor, target_layer):\n",
    "    model.zero_grad()\n",
    "    features = []\n",
    "    def hook(module, input, output):\n",
    "        features.append(output)\n",
    "    \n",
    "    hook_handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    output = model(img_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    pred_class_tensor = output[:, pred_class]\n",
    "    \n",
    "    pred_class_tensor.backward()\n",
    "    \n",
    "    grads = target_layer.weight.grad[0]\n",
    "    pooled_grads = torch.mean(grads, dim=[0, 2, 3])\n",
    "    \n",
    "    target = features[0]\n",
    "    for i in range(pooled_grads.size(0)):\n",
    "        target[:, i, :, :] *= pooled_grads[i]\n",
    "    \n",
    "    heatmap = torch.mean(target, dim=1).squeeze()\n",
    "    heatmap = F.relu(heatmap)\n",
    "    heatmap /= torch.max(heatmap)\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    return heatmap.detach().cpu().numpy()\n",
    "\n",
    "# Function to display Grad-CAM\n",
    "def display_gradcam(img, heatmap, alpha=0.4):\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.uint8(255 * img)\n",
    "    \n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = np.uint8(plt.get_cmap(\"jet\")(heatmap)[:, :, :3] * 255)\n",
    "    \n",
    "    superimposed_img = heatmap * alpha + img\n",
    "    superimposed_img = np.uint8(superimposed_img)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display random samples from the test set with Grad-CAM heatmaps\n",
    "num_images = 4\n",
    "correct_indices = [i for i, (inputs, labels) in enumerate(test_loader) if torch.round(model(inputs).squeeze()) == labels]\n",
    "incorrect_indices = [i for i, (inputs, labels) in enumerate(test_loader) if torch.round(model(inputs).squeeze()) != labels]\n",
    "\n",
    "# Display correct predictions with Grad-CAM\n",
    "if len(correct_indices) > 0:\n",
    "    print(\"Correct Predictions with Grad-CAM:\")\n",
    "    for idx in random.sample(correct_indices, min(num_images, len(correct_indices))):\n",
    "        img_tensor, _ = test_dataset[idx]\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        heatmap = get_gradcam_heatmap(model, img_tensor, model.conv3)\n",
    "        display_gradcam(img_tensor.squeeze(), heatmap)\n",
    "else:\n",
    "    print(\"No correct predictions to display.\")\n",
    "\n",
    "# Display incorrect predictions with Grad-CAM\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(\"Incorrect Predictions with Grad-CAM:\")\n",
    "    for idx in random.sample(incorrect_indices, min(num_images, len(incorrect_indices))):\n",
    "        img_tensor, _ = test_dataset[idx]\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        heatmap = get_gradcam_heatmap(model, img_tensor, model.conv3)\n",
    "        display_gradcam(img_tensor.squeeze(), heatmap)\n",
    "else:\n",
    "    print(\"No incorrect predictions to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CycleGAN Env",
   "language": "python",
   "name": "cyclegan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
