# script code 1
conda create -n image_processing_env python=3.8
conda activate image_processing_env
conda install -c conda-forge pandas scikit-learn matplotlib numpy pillow tensorflow
conda install -c anaconda ipykernel
python -m ipykernel install --user --name=image_processing_env --display-name "Python (image_processing_env)"



# script code 2
# Step 1: Create the Conda environment
conda create -n svm_image_env python=3.8

# Step 2: Activate the Conda environment
conda activate svm_image_env

# Step 3: Install required libraries
conda install -c conda-forge numpy matplotlib opencv jupyter

# Step 4: Install ipykernel in the environment
conda install -c anaconda ipykernel

# Step 5: Add the environment as a Jupyter kernel
python -m ipykernel install --user --name=svm_image_env --display-name "Python (svm_image_env)"

# Step 6: Launch Jupyter Notebook
jupyter notebook



# script code 3
conda create -n medsam python=3.10 -y
conda activate medsam
pip3 install torch torchvision torchaudio
cd script_code_3
pip install -e .

# script code 4 (https://github.com/OpenGVLab/SAM-Med2D/tree/main)
conda create -n medsam2d python=3.10 -y
conda activate medsam2d
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
conda install numpy
pip install opencv-python matplotlib
pip install 'git+https://github.com/facebookresearch/segment-anything.git'
conda install -c anaconda ipykernel
python -m ipykernel install --user --name=medsam2d --display-name "Python (medsam2d)"
pip install pydrive
pip install albumentations
pip install -U numpy
conda update albumentations scipy scikit-image
conda install -c conda-forge cupy


# script code 5 (https://github.com/AIM-Harvard/foundation-cancer-image-biomarker)
conda create -n foundation_model_ct python=3.8
conda activate foundation_model_ct
pip install foundation-cancer-image-biomarker
cd /mnt/Data1/GitHub/Code_Demo_CCIR_2024/script_code_5/
pip install -r additional_requirements.txt
pip install -U "huggingface_hub[cli]"
# Download from here
huggingface-cli download surajpaib/fmcib --local-dir '/mnt/movs/Downloads/pretrain_model/'
# Or from here
https://zenodo.org/records/10528450/files/model_weights.torch?download=1
# page for the model deployment
# https://aim-harvard.github.io/foundation-cancer-image-biomarker/replication-guide/data/#downloading-the-datasets
# https://aim-harvard.github.io/foundation-cancer-image-biomarker/getting-started/quick-start/

lighter predict --config_file ./experiments/inference/get_predictions.yaml
